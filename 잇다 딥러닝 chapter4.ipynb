{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1b1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.48382436  1.0402586  -0.37158765]\n",
      " [ 0.41325548 -0.41823927 -1.39092008]]\n",
      "[ 1.26222454  0.24773982 -1.47478066]\n",
      "0\n",
      "3.0928248376142635\n",
      "[[ 0.42035753  0.15241711 -0.57277464]\n",
      " [ 0.6305363   0.22862566 -0.85916196]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import sigmoid, softmax\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "class simpleNet:\n",
    "  def __init__(self):\n",
    "    self.W=np.random.randn(2,3) # 정규분포로 초기화\n",
    "  def predict(self,x): # 예측 수행\n",
    "    return np.dot(x, self.W)\n",
    "  def loss(self,x,t): # 손실함수의 값 구함(x는 입력데이터, t는 정답레이블)\n",
    "    z=self.predict(x) \n",
    "    y=softmax(z)\n",
    "    loss=cross_entropy_error(y,t)\n",
    "\n",
    "    return loss\n",
    "\n",
    "net=simpleNet()\n",
    "print(net.W)\n",
    "x=np.array([0.6,0.9])\n",
    "p=net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "t=np.array([0,0,1])\n",
    "print(net.loss(x,t))\n",
    "def f(W): # net.W을 인수로 받아 손실함수 계산하는 새로운 함수 f를 정의\n",
    "  return net.loss(x,t)\n",
    "dw=numerical_gradient(f,net.W) \n",
    "print(dw)\n",
    "#f=lamda w: net.loss(x,t)\n",
    "#dw=numerical_gradient(f,net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328a5d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std*\\\n",
    "                            np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*\\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'], self.params['W2']\n",
    "        b1,b2=self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "        # x:입력데이터,t:정답레이블\n",
    "        # 손실 함수 값을 계산하는 메서드\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y, axis=1)\n",
    "        t=np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "        # x:입력데이터, t:정답레이블\n",
    "        # 각 매개변수의 기울기 계산\n",
    "    def numercal_gradient(self,x,t):\n",
    "        loss_W=lambda W: self.loss(x,t)\n",
    "\n",
    "        grads={}\n",
    "        grads['W1']=numercal_gradient(loss_W, self,params['W1'])\n",
    "        grads['b1']=numercal_gradient(loss_W, self,params['b1'])\n",
    "        grads['W2']=numercal_gradient(loss_W, self,params['W2'])\n",
    "        grads['b2']=numercal_gradient(loss_W, self,params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "net=TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # params에 이 신경망에 필요한 매개변수가 모두 저장됨\n",
    "print(net.params['b1'].shape) # params에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용됨\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)\n",
    "x=np.random.rand(100,784) # 더미 입력 데이터 100장 분량 \n",
    "y=net.predict(x) # 예측 처리(순방향 처리) 실행 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9c62ac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'simpleNet' object has no attribute 'numerical_gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4828\\2199567995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 더미 정답 레이블(100장 분량)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mgrads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 기울기 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;31m# grads에는 params에 대응하는 각 매개변수의 기울기 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m# numberical_gradient()메서드를 사용해 기울기 계산 시 grads에 기울기 정보 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'simpleNet' object has no attribute 'numerical_gradient'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std*\\\n",
    "                            np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*\\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'], self.params['W2']\n",
    "        b1,b2=self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "        # x:입력데이터,t:정답레이블\n",
    "        # 손실 함수 값을 계산하는 메서드\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y, axis=1)\n",
    "        t=np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "        # x:입력데이터, t:정답레이블\n",
    "        # 각 매개변수의 기울기 계산\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W=lambda W: self.loss(x,t)\n",
    "\n",
    "        grads={}\n",
    "        grads['W1']=numercal_gradient(loss_W, self,params['W1'])\n",
    "        grads['b1']=numercal_gradient(loss_W, self,params['b1'])\n",
    "        grads['W2']=numercal_gradient(loss_W, self,params['W2'])\n",
    "        grads['b2']=numercal_gradient(loss_W, self,params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "x=np.random.rand(100,784) # 더미 입력 데이터(100장 분량)\n",
    "t=np.random.rand(100,10) # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads=net.numerical_gradient(x,t) # 기울기 계산\n",
    "# grads에는 params에 대응하는 각 매개변수의 기울기 저장\n",
    "# numberical_gradient()메서드를 사용해 기울기 계산 시 grads에 기울기 정보 저장\n",
    "grads['W1'].shape\n",
    "grads['b1'].shape\n",
    "grads['W2'].shape\n",
    "grads['b2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0190253f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4828\\1897673498.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m   \u001b[1;31m# 기울기 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m   \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m   \u001b[1;31m# grad=network.gradient(x_batch, t_batch) 성능 개선판\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4828\\1897673498.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std*\\\n",
    "                            np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*\\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'], self.params['W2']\n",
    "        b1,b2=self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "        # x:입력데이터,t:정답레이블\n",
    "        # 손실 함수 값을 계산하는 메서드\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y, axis=1)\n",
    "        t=np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "        # x:입력데이터, t:정답레이블\n",
    "        # 각 매개변수의 기울기 계산\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W=lambda W: self.loss(x,t)\n",
    "\n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_W, self,params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_W, self,params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_W, self,params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_W, self,params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "(x_train, t_train), (x_test, t_test)=\\\n",
    "  load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num=10000 # 반복횟수\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100 # 미니배치 크기\n",
    "learning_rate=0.1\n",
    "network=TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "  # 미니배치 획득\n",
    "  batch_mask=np.random.choice(train_size, batch_size)\n",
    "  x_batch=x_train[batch_mask]\n",
    "  t_batch=t_train[batch_mask]\n",
    "\n",
    "  # 기울기 계산\n",
    "  grad=network.numerical_gradient(x_batch, t_batch)\n",
    "  # grad=network.gradient(x_batch, t_batch) 성능 개선판\n",
    "\n",
    "  # 매개변수 갱신\n",
    "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network.params[key]-=learning_rate*grad[key]\n",
    "\n",
    "  #학습 경과 기록\n",
    "  loss=network.loss(x_batch, t_batch)\n",
    "  train_loss_list.append[loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4acc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfb26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c4554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
